{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In these exercises you will first be solving tasks related to the new concepts.\n",
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Given the list of pluralized words below, define your own simple word stemmer function or class,  limited to only simple rules and regex. No libraries! It should strip basic endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli', 'deni', 'itemization', 'sensational', 'reference', 'colonizer']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "plurals = [\n",
    "    \"flies\",\n",
    "    \"denied\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "]\n",
    "\n",
    "class MyStemmer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stem_word(self, word):\n",
    "        if re.match(r\".*(s|es|ed)$\", word):\n",
    "            return re.sub(r\"(s|es|ed)$\", \"\", word)\n",
    "        return word    \n",
    "\n",
    "    def stem(self, words):\n",
    "        return [self.stem_word(word) for word in words]\n",
    "\n",
    "stemmer = MyStemmer()\n",
    "stemmer.stem(plurals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After your initial implementation, run it on the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friendly', 'puzzling', 'helpful']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words = [\n",
    "    \"friendly\",\n",
    "    \"puzzling\",\n",
    "    \"helpful\",\n",
    "]\n",
    "stemmer.stem(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizing that fixing future words manually can be problematic, use a desired NLTK stemmer and run it on all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli',\n",
       " 'deni',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'friendli',\n",
       " 'puzzl',\n",
       " 'help']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "all_words = plurals + new_words\n",
    "\n",
    "def stem_wordlist(stemmer, wordlist):\n",
    "    assert isinstance(stemmer, nltk.stem.StemmerI)\n",
    "    return [stemmer.stem(word) for word in wordlist]\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stem_wordlist(stemmer, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. There are likely a few words in the outputs above that would cause issues in real-world applications. Pick some examples, and show how they are solved with a lemmatizer. Use either spaCy or nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here! Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tollef/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly\n",
      "colonizer\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"flies\"))\n",
    "print(lemmatizer.lemmatize(\"colonizer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lemmatization - Practical Example\n",
    "Using the news corpus (subset/category of the Brown corpus), perform common text normalization techniques such as stopword filtering and stemming/lemmatization. Compare the top 10 most common **words** before and after these normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a', 'in', 'for', 'that', 'is', 'was']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk; nltk.download('brown')  # ensure we have the data\n",
    "from nltk.corpus import brown\n",
    "news = brown.words(categories='news')\n",
    "\n",
    "news_words = [word.lower() for word in news if word.isalpha()]\n",
    "top10 = nltk.FreqDist(news_words).most_common(10)\n",
    "top10 = [word for word, _ in top10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'would',\n",
       " 'year',\n",
       " 'new',\n",
       " 'one',\n",
       " 'state',\n",
       " 'last',\n",
       " 'two',\n",
       " 'first',\n",
       " 'president']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = [w for w in news_words if w not in stopwords]\n",
    "words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "top10 = nltk.FreqDist(words).most_common(10)\n",
    "top10 = [word for word, _ in top10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a way to measure the importance of a word in a document.\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the term (word)\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Implement TF-IDF using NLTKs FreqDist (no use of e.g. scikit-learn and other high-level libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from math import log10\n",
    "\n",
    "\n",
    "def tf(document, term):\n",
    "    freq = nltk.FreqDist(document)\n",
    "    return freq[term]\n",
    "\n",
    "\n",
    "def idf(documents, term):\n",
    "    num_docs_with_term = 0\n",
    "    for d in documents:\n",
    "        if term in d:\n",
    "            num_docs_with_term += 1\n",
    "\n",
    "    if num_docs_with_term == 0:\n",
    "        return 0\n",
    "\n",
    "    return log10(len(documents) / num_docs_with_term)\n",
    "\n",
    "def tf_idf(all_documents, document, term):\n",
    "    _tf = tf(document, term)\n",
    "    _idf = idf(all_documents, term)\n",
    "    # print(f\"TF: {_tf}, IDF: {_idf}\")\n",
    "    return _tf * _idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With your TF-IDF function in place, calculate the TF-IDF for the following words in the first document of the news articles found in the Brown corpus: \n",
    "\n",
    "- *the*\n",
    "- *nevertheless*\n",
    "- *highway*\n",
    "- *election*\n",
    "\n",
    "Perform any preprocessing steps you deem necessary. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF of 'the': 0.00\n",
      "TF-IDF of 'nevertheless': 0.94\n",
      "TF-IDF of 'highway': 7.29\n",
      "TF-IDF of 'election': 8.43\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get fileids from news:\n",
    "fileids = brown.fileids(categories='news')\n",
    "# we make these lists as they are an NLTK corpus object\n",
    "first_doc = list(brown.words(fileids[0]))\n",
    "all_docs = [list(brown.words(fileid)) for fileid in fileids]\n",
    "\n",
    "def preprocess(document):\n",
    "    # TODO: implement some preprocessing steps.\n",
    "    pass\n",
    "\n",
    "# LF\n",
    "def preprocess(document):\n",
    "    # TODO: implement some preprocessing steps.\n",
    "    words = [re.sub(r'[^a-z]', '', w.lower()) for w in document]\n",
    "    # require length > 1\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    return words\n",
    "\n",
    "first_doc = preprocess(first_doc)\n",
    "all_docs = [preprocess(doc) for doc in all_docs]\n",
    "\n",
    "terms = [\"the\", \"nevertheless\", \"highway\", \"election\"]\n",
    "\n",
    "for term in terms:\n",
    "    score = tf_idf(all_docs, first_doc, term)\n",
    "    print(f\"TF-IDF of '{term}': {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. While TF-IDF is primarily used for information retrieval and text mining, reflect on how TF-IDF could be used in a language modeling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect an answer that includes the importance of weighing words based on their frequency and inverse frequency, as we don't want to overrepresent common words when predicting the next word. If we only considered frequency, for example, we would be left with \"a\", \"the\", \"in\", etc. as the most \"important\", and thus most predicted words.\n",
    "\n",
    "So similarly to the n-gram question in lab2, we could utilize these weights as a kind of filter on the next word predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You were previously introduced to word representations. TF-IDF can be considered one. What are some differences between the TF-IDF output and one that is computed once from a vocabulary (e.g. one-hot encoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a mapping between a word and a number (as opposed to a vector based on its position in the vocabulary). This number, instead of being a binary \"in the document\" or \"not in the document\", is a real number that represents the importance of the word in the given context, based on the entire collection. \n",
    "\n",
    "Put simply, TF-IDF is a measure of importance, where one-hot encoding is a measure of presence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Practical Example\n",
    "You will again be looking at specific words for a document, but this time weighted by their TF-IDF scores. Ideally, the scoring should be able to retrieve representative words for this document in context of its document collection or category.\n",
    "\n",
    "You will do the following:\n",
    "- Select a category from the Reuters (news) corpus\n",
    "- Perform preprocessing\n",
    "- Calculate TF-IDF scores\n",
    "- Find the top 5 words for a subset of documents in your collection (e.g. 5, 10, ..)\n",
    "- Inspect whether these words make sense for a given document, and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australian foreign ship ban ends but nsw ports hit tug crews in new south wales nsw victoria and western australia yesterday lifted their ban on foreign flag ships carrying containers but nsw ports are still being disrupted by separate dispute shipping sources said the ban imposed week ago over pay claim had prevented the movement in or out of port of nearly vessels they said the pay dispute went before hearing of the arbitration commission today meanwhile disruption began today to cargo handling in the ports of sydney newcastle and port kembla they said the industrial action at the nsw ports is part of the week of action called by the nsw trades and labour council to protest changes to the state workers compensation laws the shipping sources said the various port unions appear to be taking it in turn to work for short time at the start of each shift and then to walk off cargo handling in the ports has been disrupted with container movements most affected but has not stopped altogether they said they said they could not say how long the disruption will go on and what effect it will have on shipping movements\n",
      "nsw: 9.27\n",
      "ban: 5.56\n",
      "ports: 4.83\n",
      "disruption: 3.71\n",
      "disrupted: 3.36\n",
      "\n",
      "australian foreign ship ban ends tug crews in new south wales nsw victoria and western australia yesterday lifted their ban on foreign flag ships carrying containers but nsw ports are still being disrupted by separate dispute shipping sources said the ban imposed week ago over pay claim had prevented the movement in or out of port of nearly vessels they said the pay dispute went before hearing of the arbitration commission today meanwhile disruption began today to cargo handling in the ports of sydney newcastle and port kembla they said the industrial action at the nsw ports is part of the week of action called by the nsw trades and labour council to protest changes to the state workers compensation laws\n",
      "nsw: 7.42\n",
      "ban: 5.56\n",
      "ports: 2.90\n",
      "kembla: 2.16\n",
      "dispute: 2.12\n",
      "\n",
      "independent chairman for dutch cargo dispute the two sides in the rotterdam port general cargo dispute have agreed to appoint an independent chairman han lammers to preside over future meetings employers spokesman gerard zeebregts said lammers queen commissioner for the province of flevoland will not act as mediator but will draw up an agenda and procedures for meetings between the employers and unions on work practice agreement and proposed redundancies two months of strikes in the sector began on january in protest at employers proposals for redundancies from the strong workforce this year the strikes were called off by the main port union fnv on march following an amsterdam court interim injunction against the redundancies on procedural grounds the court is due to make final ruling on may but zeebregts said he expected the judgment to go against the employers and they were therefore very likely to restart the complicated legal redundancy procedures in the near future meanwhile the dispute over new work practice agreement in the port grain sector continued with maintenance workers on strike although loading was not affected spokesman for graan elevator mij the largest employer in the sector said the employers have written to the union asking it to reconsider its position and meeting of union members has been called for tomorrow\n",
      "employers: 6.26\n",
      "lammers: 4.91\n",
      "redundancies: 4.37\n",
      "zeebregts: 3.96\n",
      "procedures: 3.96\n",
      "\n",
      "todd shipyards lt tod struck on west coast todd shipyards corp said production workers represented by the multi union pacific coast metal trades district council at its san francisco division struck on april six it said negotiations are expected to resume at the end of this month todd also said the collective bargaining division in effect at its galveston division expires april and negotiations with the galveston metal trades council are continuing the company said results of balloting on new collective bargaining agreement proposal in its seattle division are expected to be tabulated at the close of business tomorrow the pacific coast council has recommended acceptance of that proposal by membership todd said\n",
      "todd: 9.83\n",
      "division: 9.83\n",
      "council: 5.03\n",
      "bargaining: 4.91\n",
      "galveston: 4.91\n",
      "\n",
      "london freight market features grain out of moderately active grain fixing was reported out of the but none of the business involved the significant voyages to the continent or japan ship brokers said steady dlrs was paid from the gulf to morocco and dlrs was paid for long tons from the gulf to taiwan vessel carrying long tons of bagged wheat flour from the gulf to aqaba received lump sum of dlrs grain from the great lakes to algeria made dlrs against paid for similar fixing towards the end of march market talk suggested federal commerce vessel had been booked to move grain from the great lakes to morocco on comanav account at about dlrs and had been paid for cargo of oilseeds from british columbia to japan but no confirmation was obtainable on the continent shippers agreed dlrs for wheat from la pallice to buenaventura and dlrs for grain from ghent to naples venice range elsewhere maize from east london to japan paid dlrs soviet charterers reappeared in the timecharter sector and secured tonner from savona for trans atlantic round trip at dlrs daily and tonner from antwerp hamburg for similar voyage at dlrs daily\n",
      "paid: 9.27\n",
      "dlrs: 8.44\n",
      "grain: 4.97\n",
      "tonner: 4.31\n",
      "fixing: 3.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import nltk; nltk.download(\"reuters\")\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# TODO: select a category of the Reuters corpus\n",
    "# (hint: check with reuters.categories())\n",
    "#\n",
    "# Apply preprocessing whenever it suits you :-)\n",
    "files = reuters.fileids(categories=[\"ship\"])\n",
    "docs = [reuters.words(fileid) for fileid in files]\n",
    "docs = [preprocess(doc) for doc in docs]\n",
    "\n",
    "# TODO: calculate the TF-IDF scores for the selected documents\n",
    "TOP_N = 5\n",
    "SUBSET_SIZE = 5\n",
    "for doc_id, doc in list(zip(files, docs))[:SUBSET_SIZE]:\n",
    "    print(\" \".join(doc))\n",
    "    scores = {term: tf_idf(docs, doc, term) for term in doc}\n",
    "    # sorted\n",
    "    scores = {\n",
    "        k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "    for term, score in list(scores.items())[:TOP_N]:\n",
    "        print(f\"{term}: {score:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe your understanding of POS tagging and its possible use-cases in context of text generation applications/language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much alike TF-IDF as a weighing scheme, including POS tagging for language modeling could be used to ensure proper sentence structure (such as Subject-Verb-Object, depending on language, of course) and in this way filter out unlikely word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a UnigramTagger (NLTK) using the Brown corpus. \n",
    "Hint: the taggers in nltk require a list of sentences containing tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "corpus = brown.tagged_sents()\n",
    "\n",
    "tagger = nltk.UnigramTagger(train=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use this tagger to tag the text given below. Print out the POS tags for all variants of \"justify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "justify (VB)\n",
      "justifier (None)\n",
      "justifications (NNS)\n",
      "justifying (VBG)\n",
      "justify (VB)\n",
      "justifier (None)\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Imagine a situation where you have to explain why you did something â€“ that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\n",
    "\"\"\"\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "tagged = tagger.tag(words)\n",
    "for word, tag in tagged:\n",
    "    if \"just\" in word:\n",
    "        print(f\"{word} ({tag})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Your results may be disappointing. Repeat the same task as above using both the default NLTK pos-tagger and with spaCy. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "justify (VBP)\n",
      "justifier (NN)\n",
      "justifications (NNS)\n",
      "justifying (VBG)\n",
      "justify (VBP)\n",
      "justifier (NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "for word, tag in tagged:\n",
    "    if \"just\" in word:\n",
    "        print(f\"{word} ({tag})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "justify (VERB)\n",
      "justifier (NOUN)\n",
      "justifications (NOUN)\n",
      "justifying (VERB)\n",
      "justify (VERB)\n",
      "justifier (NOUN)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if \"just\" in token.text:\n",
    "        print(f\"{token.text} ({token.pos_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finally, explore more features of the what the spaCy *document* includes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect the student to explore attributes such as lemmatization, perhaps named entity recognition, and other easily accessible features of the document object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
