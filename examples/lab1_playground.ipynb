{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 playground\n",
    "Below are a few examples you can run if you wish! These make use of the `transformers` library.\n",
    "The examples require around ~1 GB of available storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements:\n",
    "# %pip install transformers\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-125m's vocab size is 50257\n",
      "{'input_ids': tensor([[1135, 2050,  780]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "Inspect a vocab id 1135: We\n",
      "The outputs of the next token will be a distribution over the entire vocabulary\n",
      "Output: 50257 == Vocab: 50257\n",
      "--------------------------------------------------\n",
      "We study because\n",
      "- of (prob 0.2096)\n",
      "- it (prob 0.1508)\n",
      "- we (prob 0.0938)\n",
      "- the (prob 0.0833)\n",
      "- they (prob 0.0553)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "print(f\"{model_id}'s vocab size is {model.config.vocab_size}\")\n",
    "\n",
    "query = \"We study because\"\n",
    "# this encodes the query according to the model's setup and vocabulary\n",
    "encoded_text = tokenizer(query, return_tensors=\"pt\")\n",
    "print(encoded_text)  # notice the input_ids, which are the token indices in the vocab\n",
    "print(f\"Inspect a vocab id 1135: {tokenizer.decode(1135)}\")\n",
    "\n",
    "with torch.no_grad():  # disable gradient calculation for inference\n",
    "    outputs = model(**encoded_text)\n",
    "next_token = outputs.logits[-1, -1, :] # select the last token from the sequence and all the vocab logits (:)\n",
    "print(\"The outputs of the next token will be a distribution over the entire vocabulary\")\n",
    "print(f\"Output: {next_token.shape[0]} == Vocab: {model.config.vocab_size}\")\n",
    "\n",
    "probdist = torch.softmax(next_token, -1)  # -1 means last axis, or the vocab axis\n",
    "\n",
    "k_best = 5\n",
    "topk_next_tokens= torch.topk(probdist, k_best)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(query)\n",
    "for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values):\n",
    "    print(f\"- {tokenizer.decode(idx).strip()} (prob {prob.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can also be used in a simple pipeline:\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt_pipe = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125m\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make waffles?\n",
      "\n",
      "Waffles are made of flour, sugar, and salt. The ingredients in the batter are the same as those in a waffle iron, but the ingredients are different. In the recipe, I’m going to use sugar. I use a teaspoonful of sugar for the dough. If you want to make your own, you can use powdered sugar instead of the sugar you use when making your batter. You can also use the powdered ingredient in your\n"
     ]
    }
   ],
   "source": [
    "def pred(text, max_tok=100):\n",
    "    out = gpt_pipe(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_tok,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=2,\n",
    "        early_stopping=True,\n",
    "        length_penalty=0.5,\n",
    "        use_cache=True,\n",
    "        pad_token_id=50256,\n",
    "    )\n",
    "    print(out[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "pred(\"How do I make waffles?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe for delicious waffles. Ingredients:\n",
      "\n",
      "1/2 cup (1 stick) unsalted butter, softened\n",
      "(about 1/4 cup)\n",
      "¼ cup plus 2 tablespoons brown sugar\n",
      "or 1 teaspoon vanilla extract\n",
      "and 1 cup confectioners' sugar, plus more for sprinkling on top of the batter.\n",
      "Preheat the oven to 350°F. Place the butter in a bowl and set aside. In a large bowl, whisk together the melted butter and the brown\n"
     ]
    }
   ],
   "source": [
    "pred(\"Recipe for delicious waffles. Ingredients:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A BERT masked example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At NTNU you can study _ engineering\n",
      "- electrical\n",
      "- civil\n",
      "- mechanical\n",
      "- chemical\n",
      "- software\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "# avoid transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "bert_model_id = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_id)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model_id)\n",
    "\n",
    "MASK = tokenizer.mask_token\n",
    "def bert(text):\n",
    "    text = text.replace(\"_\", MASK)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    tokens = tokenizer.tokenize(decoded)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_ids)[0]\n",
    "\n",
    "    _, top_k_indices = torch.topk(predictions[0, tokens.index(MASK)], k=5)\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "    print(text.replace(MASK, \"_\"))\n",
    "    for token in top_k_tokens:\n",
    "        print(f\"- {token}\")\n",
    "    \n",
    "bert(\"At NTNU you can study _ engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
