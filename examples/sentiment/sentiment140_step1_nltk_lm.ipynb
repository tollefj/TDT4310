{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple ngram language model\n",
    "From the twitter sentiment based data (see [step 0](sentiment140_step0_parse_data.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"sentiment140_train.csv\"\n",
    "df = pd.read_csv(path)\n",
    "texts = df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [tt.tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1305022"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)  # 1.3 million tweets after filtering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'awww',\n",
       " \"that's\",\n",
       " 'a',\n",
       " 'bummer',\n",
       " 'you',\n",
       " 'shoulda',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'of',\n",
       " 'third',\n",
       " 'day',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " '</s>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(pad_both_ends(corpus[0], n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'awww'),\n",
       " ('<s>', 'awww', \"that's\"),\n",
       " ('awww',),\n",
       " ('awww', \"that's\"),\n",
       " ('awww', \"that's\", 'a'),\n",
       " (\"that's\",),\n",
       " (\"that's\", 'a'),\n",
       " (\"that's\", 'a', 'bummer'),\n",
       " ('a',),\n",
       " ('a', 'bummer'),\n",
       " ('a', 'bummer', 'you'),\n",
       " ('bummer',),\n",
       " ('bummer', 'you'),\n",
       " ('bummer', 'you', 'shoulda'),\n",
       " ('you',),\n",
       " ('you', 'shoulda'),\n",
       " ('you', 'shoulda', 'got'),\n",
       " ('shoulda',),\n",
       " ('shoulda', 'got'),\n",
       " ('shoulda', 'got', 'david'),\n",
       " ('got',),\n",
       " ('got', 'david'),\n",
       " ('got', 'david', 'carr'),\n",
       " ('david',),\n",
       " ('david', 'carr'),\n",
       " ('david', 'carr', 'of'),\n",
       " ('carr',),\n",
       " ('carr', 'of'),\n",
       " ('carr', 'of', 'third'),\n",
       " ('of',),\n",
       " ('of', 'third'),\n",
       " ('of', 'third', 'day'),\n",
       " ('third',),\n",
       " ('third', 'day'),\n",
       " ('third', 'day', 'to'),\n",
       " ('day',),\n",
       " ('day', 'to'),\n",
       " ('day', 'to', 'do'),\n",
       " ('to',),\n",
       " ('to', 'do'),\n",
       " ('to', 'do', 'it'),\n",
       " ('do',),\n",
       " ('do', 'it'),\n",
       " ('do', 'it', '</s>'),\n",
       " ('it',),\n",
       " ('it', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "padded_bigrams = list(pad_both_ends(corpus[0], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "max_n = 3\n",
    "train, vocab = padded_everygram_pipeline(max_n, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE, Laplace\n",
    "lm = MLE(max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 251406 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('studying', 'in', 'trondheim')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup([\"studying\", \"in\", \"trondheim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('studying', 'in', '<UNK>')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup([\"studying\", \"in\", \"gjÃ¸vik\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16148"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[\"hey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'i': 1253, 'you': 534, 'there': 439, 'hey': 367, 'guys': 343, \"i'm\": 309, 'thanks': 295, 'how': 274, 'girl': 217, 'just': 175, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[['hey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14400196254804154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prob for \"you\" preceded by \"are\", i.e. \"are you\"\n",
    "lm.score(\"you\", [\"are\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.193230105130112\n",
      "18.293130828360948\n"
     ]
    }
   ],
   "source": [
    "test = [(\"hey\", \"you\"), (\"where\", \"are\")]\n",
    "print(lm.entropy(test))\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'up': 681, 'the': 531, 'wrong': 492, 'going': 381, 'your': 266, 'happening': 126, 'that': 119, 'with': 117, 'good': 89, 'a': 83, ...})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[[\"what's\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm going to [travel through germany]\n"
     ]
    }
   ],
   "source": [
    "text = \"i'm going to\"\n",
    "\n",
    "def pred(text, n=2):\n",
    "    tokens = text.split()\n",
    "    pred = lm.generate(n, text_seed=tokens)\n",
    "    return \" \".join(w for w in pred if \">\" not in w)\n",
    "\n",
    "prediction = pred(text, n=3)\n",
    "print(f\"{text} [{prediction}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29f1e52c0d8d5e5ede6aaca4be8238d35b46afd62a3b8286547e2768de775769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
